Inverse Reinforcement Learning (IRL), as described by Andrew Ng and Stuart Russell in 2000, reverses the problem and instead tries to extract the reward function from an agent's observed behavior.IRL is about determining an unknown reward function for an MDP, given an optimal policy for that MDP.

### Reward function approximation for Gridworld Gym Environment using Inverse RL

##  Value Iteration (8x8) - Ground Truth
![Value Iteration Policy](https://raw.githubusercontent.com/Tsili123/Inverse-Reinforcement-Learning-DiT-UoA/main/inv_rl_Aris_Tsilifonis_1115201700170/value_iteration_policy.png)

## Linear programming (8x8)  
![Plot](https://raw.githubusercontent.com/Tsili123/Inverse-Reinforcement-Learning-DiT-UoA/main/inv_rl_Aris_Tsilifonis_1115201700170/linear_irl.png)

## Max entropy IRL (8x8)  
![Maximum Entropy Small IRL](https://github.com/Tsili123/Inverse-Reinforcement-Learning-DiT-UoA/blob/main/inv_rl_Aris_Tsilifonis_1115201700170/MaxEntSmallGrid.PNG)

## Max entropy IRL (25x25)     
![Maximum Entropy IRL](https://raw.githubusercontent.com/Tsili123/Inverse-Reinforcement-Learning-DiT-UoA/main/inv_rl_Aris_Tsilifonis_1115201700170/maximumentropy_irl.png)

## Max entropy Deep IRL (25x25) Com: Goal states correct! Better accuracy than maximum entropy reward map 
![Deep RL Illustration](https://raw.githubusercontent.com/Tsili123/Inverse-Reinforcement-Learning-DiT-UoA/main/inv_rl_Aris_Tsilifonis_1115201700170/DeepRLPic.PNG)




