Inverse Reinforcement Learning (IRL), as described by Andrew Ng and Stuart Russell in 2000, reverses the problem and instead tries to extract the reward function from an agent's observed behavior.IRL is about determining an unknown reward function for an MDP, given an optimal policy for that MDP.


![Plot](https://raw.githubusercontent.com/Tsili123/Inverse-Reinforcement-Learning-DiT-UoA/main/inv_rl_Aris_Tsilifonis_1115201700170/linear_irl.png)

![Value Iteration Policy](https://raw.githubusercontent.com/Tsili123/Inverse-Reinforcement-Learning-DiT-UoA/main/inv_rl_Aris_Tsilifonis_1115201700170/value_iteration_policy.png)

![Maximum Entropy IRL](https://raw.githubusercontent.com/Tsili123/Inverse-Reinforcement-Learning-DiT-UoA/main/inv_rl_Aris_Tsilifonis_1115201700170/maximumentropy_irl.png)

![Deep RL Illustration](https://raw.githubusercontent.com/Tsili123/Inverse-Reinforcement-Learning-DiT-UoA/main/inv_rl_Aris_Tsilifonis_1115201700170/DeepRLPic.PNG)




